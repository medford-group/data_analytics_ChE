{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Visualization-of-features\" data-toc-modified-id=\"Visualization-of-features-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Visualization of features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion:-Why-might-there-by-bimodal-distributions-in-a-chemical-process?\" data-toc-modified-id=\"Discussion:-Why-might-there-by-bimodal-distributions-in-a-chemical-process?-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Discussion: Why might there by bimodal distributions in a chemical process?</a></span></li></ul></li><li><span><a href=\"#Scaling-Features-and-Outputs\" data-toc-modified-id=\"Scaling-Features-and-Outputs-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Scaling Features and Outputs</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion:-What-could-go-wrong-with-rescaling-or-mean-scaling?\" data-toc-modified-id=\"Discussion:-What-could-go-wrong-with-rescaling-or-mean-scaling?-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Discussion: What could go wrong with rescaling or mean scaling?</a></span></li></ul></li><li><span><a href=\"#Multi-Linear-Regression\" data-toc-modified-id=\"Multi-Linear-Regression-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Multi-Linear Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion:-How-many-features-would-result-if-third-order-interactions-were-considered?\" data-toc-modified-id=\"Discussion:-How-many-features-would-result-if-third-order-interactions-were-considered?-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Discussion: How many features would result if third-order interactions were considered?</a></span></li></ul></li><li><span><a href=\"#Dimensionality-Reduction\" data-toc-modified-id=\"Dimensionality-Reduction-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Dimensionality Reduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Forward-Selection\" data-toc-modified-id=\"Forward-Selection-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Forward Selection</a></span></li><li><span><a href=\"#Exercise:-Use-forward-selection-to-determine-the-minimum-number-of-features-needed-to-get-an-$r^2=0.65$.\" data-toc-modified-id=\"Exercise:-Use-forward-selection-to-determine-the-minimum-number-of-features-needed-to-get-an-$r^2=0.65$.-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Exercise: Use forward selection to determine the minimum number of features needed to get an $r^2=0.65$.</a></span></li></ul></li><li><span><a href=\"#Principal-Component-Regression\" data-toc-modified-id=\"Principal-Component-Regression-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Principal Component Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion:-Why-is-the-model-with-principal-components-not-always-better-than-direct-linear-regression?\" data-toc-modified-id=\"Discussion:-Why-is-the-model-with-principal-components-not-always-better-than-direct-linear-regression?-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Discussion: Why is the model with principal components not always better than direct linear regression?</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-dimensional Data\n",
    "\n",
    "So far we have only worked with datasets that have a single input dimension. We have generated \"features\" from this dimension, but we have not considered the case of a problem where multiple inputs are given. This is a very common scenario, and one of the main advantages of many machine-learning methods is that they work well for \"high-dimesional\" data, or data with many features.\n",
    "\n",
    "In this lecture we will work with a dataset of chemical process data provided by Dow Chemical. The data comes from a generic chemical process with the following setup:\n",
    "\n",
    "<img src=\"images/dow_process.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a number of operating conditions for each of the units in the process, as well as the concentration of impurities in the output stream. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>x1:Primary Column Reflux Flow</th>\n",
       "      <th>x2:Primary Column Tails Flow</th>\n",
       "      <th>x3:Input to Primary Column Bed 3 Flow</th>\n",
       "      <th>x4:Input to Primary Column Bed 2 Flow</th>\n",
       "      <th>x5:Primary Column Feed Flow from Feed Column</th>\n",
       "      <th>x6:Primary Column Make Flow</th>\n",
       "      <th>x7:Primary Column Base Level</th>\n",
       "      <th>x8:Primary Column Reflux Drum Pressure</th>\n",
       "      <th>x9:Primary Column Condenser Reflux Drum Level</th>\n",
       "      <th>...</th>\n",
       "      <th>x36: Feed Column Recycle Flow</th>\n",
       "      <th>x37: Feed Column Tails Flow to Primary Column</th>\n",
       "      <th>x38: Feed Column Calculated DP</th>\n",
       "      <th>x39: Feed Column Steam Flow</th>\n",
       "      <th>x40: Feed Column Tails Flow</th>\n",
       "      <th>Avg_Reactor_Outlet_Impurity</th>\n",
       "      <th>Avg_Delta_Composition Primary Column</th>\n",
       "      <th>y:Impurity</th>\n",
       "      <th>Primary Column Reflux/Feed Ratio</th>\n",
       "      <th>Primary Column Make/Reflux Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-01 00:00:00</td>\n",
       "      <td>327.813</td>\n",
       "      <td>45.7920</td>\n",
       "      <td>2095.06</td>\n",
       "      <td>2156.01</td>\n",
       "      <td>98.5005</td>\n",
       "      <td>95.4674</td>\n",
       "      <td>54.3476</td>\n",
       "      <td>41.0121</td>\n",
       "      <td>52.2353</td>\n",
       "      <td>...</td>\n",
       "      <td>62.8707</td>\n",
       "      <td>45.0085</td>\n",
       "      <td>66.6604</td>\n",
       "      <td>8.68813</td>\n",
       "      <td>99.9614</td>\n",
       "      <td>5.38024</td>\n",
       "      <td>1.49709</td>\n",
       "      <td>1.77833</td>\n",
       "      <td>3.32803</td>\n",
       "      <td>0.291226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-01 01:00:00</td>\n",
       "      <td>322.970</td>\n",
       "      <td>46.1643</td>\n",
       "      <td>2101.00</td>\n",
       "      <td>2182.90</td>\n",
       "      <td>98.0014</td>\n",
       "      <td>94.9673</td>\n",
       "      <td>54.2247</td>\n",
       "      <td>41.0076</td>\n",
       "      <td>52.5378</td>\n",
       "      <td>...</td>\n",
       "      <td>62.8651</td>\n",
       "      <td>45.0085</td>\n",
       "      <td>66.5496</td>\n",
       "      <td>8.70683</td>\n",
       "      <td>99.8637</td>\n",
       "      <td>5.33345</td>\n",
       "      <td>1.51392</td>\n",
       "      <td>1.76964</td>\n",
       "      <td>3.29556</td>\n",
       "      <td>0.294044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-01 02:00:00</td>\n",
       "      <td>319.674</td>\n",
       "      <td>45.9927</td>\n",
       "      <td>2102.96</td>\n",
       "      <td>2151.39</td>\n",
       "      <td>98.8229</td>\n",
       "      <td>96.0785</td>\n",
       "      <td>54.6130</td>\n",
       "      <td>41.0451</td>\n",
       "      <td>52.0159</td>\n",
       "      <td>...</td>\n",
       "      <td>62.8656</td>\n",
       "      <td>45.0085</td>\n",
       "      <td>66.0599</td>\n",
       "      <td>8.69269</td>\n",
       "      <td>100.2490</td>\n",
       "      <td>5.37677</td>\n",
       "      <td>1.50634</td>\n",
       "      <td>1.76095</td>\n",
       "      <td>3.23481</td>\n",
       "      <td>0.300552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-01 03:00:00</td>\n",
       "      <td>327.223</td>\n",
       "      <td>46.0960</td>\n",
       "      <td>2101.37</td>\n",
       "      <td>2172.14</td>\n",
       "      <td>98.7733</td>\n",
       "      <td>96.1223</td>\n",
       "      <td>54.9153</td>\n",
       "      <td>41.0405</td>\n",
       "      <td>52.9477</td>\n",
       "      <td>...</td>\n",
       "      <td>62.8669</td>\n",
       "      <td>45.0085</td>\n",
       "      <td>67.9697</td>\n",
       "      <td>8.70482</td>\n",
       "      <td>100.3200</td>\n",
       "      <td>5.32315</td>\n",
       "      <td>1.47935</td>\n",
       "      <td>1.75226</td>\n",
       "      <td>3.31287</td>\n",
       "      <td>0.293752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-12-01 04:00:00</td>\n",
       "      <td>331.177</td>\n",
       "      <td>45.8493</td>\n",
       "      <td>2114.06</td>\n",
       "      <td>2157.77</td>\n",
       "      <td>99.3231</td>\n",
       "      <td>94.7521</td>\n",
       "      <td>54.0925</td>\n",
       "      <td>40.9934</td>\n",
       "      <td>53.0507</td>\n",
       "      <td>...</td>\n",
       "      <td>62.8673</td>\n",
       "      <td>45.0085</td>\n",
       "      <td>67.6454</td>\n",
       "      <td>8.70077</td>\n",
       "      <td>100.6590</td>\n",
       "      <td>5.28227</td>\n",
       "      <td>1.44489</td>\n",
       "      <td>1.74357</td>\n",
       "      <td>3.33435</td>\n",
       "      <td>0.286107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-12-01 05:00:00</td>\n",
       "      <td>328.884</td>\n",
       "      <td>46.0729</td>\n",
       "      <td>2100.26</td>\n",
       "      <td>2134.76</td>\n",
       "      <td>99.3376</td>\n",
       "      <td>95.4188</td>\n",
       "      <td>53.9989</td>\n",
       "      <td>41.0217</td>\n",
       "      <td>53.0389</td>\n",
       "      <td>...</td>\n",
       "      <td>62.8690</td>\n",
       "      <td>45.0085</td>\n",
       "      <td>67.6828</td>\n",
       "      <td>8.69795</td>\n",
       "      <td>100.8260</td>\n",
       "      <td>5.28510</td>\n",
       "      <td>1.51144</td>\n",
       "      <td>1.73488</td>\n",
       "      <td>3.31077</td>\n",
       "      <td>0.290129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-12-01 06:00:00</td>\n",
       "      <td>327.335</td>\n",
       "      <td>46.0581</td>\n",
       "      <td>2101.57</td>\n",
       "      <td>2191.37</td>\n",
       "      <td>98.9044</td>\n",
       "      <td>94.9811</td>\n",
       "      <td>54.0685</td>\n",
       "      <td>41.0499</td>\n",
       "      <td>52.8279</td>\n",
       "      <td>...</td>\n",
       "      <td>62.8720</td>\n",
       "      <td>45.0085</td>\n",
       "      <td>66.0828</td>\n",
       "      <td>8.70780</td>\n",
       "      <td>100.3580</td>\n",
       "      <td>5.35512</td>\n",
       "      <td>1.51096</td>\n",
       "      <td>1.72619</td>\n",
       "      <td>3.30961</td>\n",
       "      <td>0.290165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-12-01 07:00:00</td>\n",
       "      <td>329.935</td>\n",
       "      <td>45.9708</td>\n",
       "      <td>2099.27</td>\n",
       "      <td>2133.95</td>\n",
       "      <td>99.6756</td>\n",
       "      <td>94.8352</td>\n",
       "      <td>54.0001</td>\n",
       "      <td>40.9886</td>\n",
       "      <td>52.7697</td>\n",
       "      <td>...</td>\n",
       "      <td>62.8694</td>\n",
       "      <td>45.0085</td>\n",
       "      <td>67.5438</td>\n",
       "      <td>8.69391</td>\n",
       "      <td>101.1360</td>\n",
       "      <td>5.31343</td>\n",
       "      <td>1.51180</td>\n",
       "      <td>1.71750</td>\n",
       "      <td>3.31009</td>\n",
       "      <td>0.287436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-12-01 08:00:00</td>\n",
       "      <td>329.128</td>\n",
       "      <td>45.8875</td>\n",
       "      <td>2099.12</td>\n",
       "      <td>2055.11</td>\n",
       "      <td>98.8823</td>\n",
       "      <td>95.0573</td>\n",
       "      <td>53.9876</td>\n",
       "      <td>41.0169</td>\n",
       "      <td>52.8802</td>\n",
       "      <td>...</td>\n",
       "      <td>62.8690</td>\n",
       "      <td>45.0085</td>\n",
       "      <td>66.9394</td>\n",
       "      <td>8.70810</td>\n",
       "      <td>100.3630</td>\n",
       "      <td>5.35183</td>\n",
       "      <td>1.48168</td>\n",
       "      <td>1.70881</td>\n",
       "      <td>3.32848</td>\n",
       "      <td>0.288816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015-12-01 09:00:00</td>\n",
       "      <td>327.686</td>\n",
       "      <td>45.8192</td>\n",
       "      <td>2109.75</td>\n",
       "      <td>2185.82</td>\n",
       "      <td>98.8448</td>\n",
       "      <td>95.5414</td>\n",
       "      <td>54.0806</td>\n",
       "      <td>41.0029</td>\n",
       "      <td>53.0875</td>\n",
       "      <td>...</td>\n",
       "      <td>62.8690</td>\n",
       "      <td>45.0085</td>\n",
       "      <td>65.5845</td>\n",
       "      <td>8.69685</td>\n",
       "      <td>100.2790</td>\n",
       "      <td>5.31385</td>\n",
       "      <td>1.51268</td>\n",
       "      <td>1.70012</td>\n",
       "      <td>3.31516</td>\n",
       "      <td>0.291564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date  x1:Primary Column Reflux Flow  \\\n",
       "0 2015-12-01 00:00:00                        327.813   \n",
       "1 2015-12-01 01:00:00                        322.970   \n",
       "2 2015-12-01 02:00:00                        319.674   \n",
       "3 2015-12-01 03:00:00                        327.223   \n",
       "4 2015-12-01 04:00:00                        331.177   \n",
       "5 2015-12-01 05:00:00                        328.884   \n",
       "6 2015-12-01 06:00:00                        327.335   \n",
       "7 2015-12-01 07:00:00                        329.935   \n",
       "8 2015-12-01 08:00:00                        329.128   \n",
       "9 2015-12-01 09:00:00                        327.686   \n",
       "\n",
       "   x2:Primary Column Tails Flow  x3:Input to Primary Column Bed 3 Flow  \\\n",
       "0                       45.7920                                2095.06   \n",
       "1                       46.1643                                2101.00   \n",
       "2                       45.9927                                2102.96   \n",
       "3                       46.0960                                2101.37   \n",
       "4                       45.8493                                2114.06   \n",
       "5                       46.0729                                2100.26   \n",
       "6                       46.0581                                2101.57   \n",
       "7                       45.9708                                2099.27   \n",
       "8                       45.8875                                2099.12   \n",
       "9                       45.8192                                2109.75   \n",
       "\n",
       "   x4:Input to Primary Column Bed 2 Flow  \\\n",
       "0                                2156.01   \n",
       "1                                2182.90   \n",
       "2                                2151.39   \n",
       "3                                2172.14   \n",
       "4                                2157.77   \n",
       "5                                2134.76   \n",
       "6                                2191.37   \n",
       "7                                2133.95   \n",
       "8                                2055.11   \n",
       "9                                2185.82   \n",
       "\n",
       "   x5:Primary Column Feed Flow from Feed Column  x6:Primary Column Make Flow  \\\n",
       "0                                       98.5005                      95.4674   \n",
       "1                                       98.0014                      94.9673   \n",
       "2                                       98.8229                      96.0785   \n",
       "3                                       98.7733                      96.1223   \n",
       "4                                       99.3231                      94.7521   \n",
       "5                                       99.3376                      95.4188   \n",
       "6                                       98.9044                      94.9811   \n",
       "7                                       99.6756                      94.8352   \n",
       "8                                       98.8823                      95.0573   \n",
       "9                                       98.8448                      95.5414   \n",
       "\n",
       "   x7:Primary Column Base Level  x8:Primary Column Reflux Drum Pressure  \\\n",
       "0                       54.3476                                 41.0121   \n",
       "1                       54.2247                                 41.0076   \n",
       "2                       54.6130                                 41.0451   \n",
       "3                       54.9153                                 41.0405   \n",
       "4                       54.0925                                 40.9934   \n",
       "5                       53.9989                                 41.0217   \n",
       "6                       54.0685                                 41.0499   \n",
       "7                       54.0001                                 40.9886   \n",
       "8                       53.9876                                 41.0169   \n",
       "9                       54.0806                                 41.0029   \n",
       "\n",
       "   x9:Primary Column Condenser Reflux Drum Level  ...  \\\n",
       "0                                        52.2353  ...   \n",
       "1                                        52.5378  ...   \n",
       "2                                        52.0159  ...   \n",
       "3                                        52.9477  ...   \n",
       "4                                        53.0507  ...   \n",
       "5                                        53.0389  ...   \n",
       "6                                        52.8279  ...   \n",
       "7                                        52.7697  ...   \n",
       "8                                        52.8802  ...   \n",
       "9                                        53.0875  ...   \n",
       "\n",
       "   x36: Feed Column Recycle Flow  \\\n",
       "0                        62.8707   \n",
       "1                        62.8651   \n",
       "2                        62.8656   \n",
       "3                        62.8669   \n",
       "4                        62.8673   \n",
       "5                        62.8690   \n",
       "6                        62.8720   \n",
       "7                        62.8694   \n",
       "8                        62.8690   \n",
       "9                        62.8690   \n",
       "\n",
       "   x37: Feed Column Tails Flow to Primary Column  \\\n",
       "0                                        45.0085   \n",
       "1                                        45.0085   \n",
       "2                                        45.0085   \n",
       "3                                        45.0085   \n",
       "4                                        45.0085   \n",
       "5                                        45.0085   \n",
       "6                                        45.0085   \n",
       "7                                        45.0085   \n",
       "8                                        45.0085   \n",
       "9                                        45.0085   \n",
       "\n",
       "   x38: Feed Column Calculated DP  x39: Feed Column Steam Flow  \\\n",
       "0                         66.6604                      8.68813   \n",
       "1                         66.5496                      8.70683   \n",
       "2                         66.0599                      8.69269   \n",
       "3                         67.9697                      8.70482   \n",
       "4                         67.6454                      8.70077   \n",
       "5                         67.6828                      8.69795   \n",
       "6                         66.0828                      8.70780   \n",
       "7                         67.5438                      8.69391   \n",
       "8                         66.9394                      8.70810   \n",
       "9                         65.5845                      8.69685   \n",
       "\n",
       "   x40: Feed Column Tails Flow  Avg_Reactor_Outlet_Impurity  \\\n",
       "0                      99.9614                      5.38024   \n",
       "1                      99.8637                      5.33345   \n",
       "2                     100.2490                      5.37677   \n",
       "3                     100.3200                      5.32315   \n",
       "4                     100.6590                      5.28227   \n",
       "5                     100.8260                      5.28510   \n",
       "6                     100.3580                      5.35512   \n",
       "7                     101.1360                      5.31343   \n",
       "8                     100.3630                      5.35183   \n",
       "9                     100.2790                      5.31385   \n",
       "\n",
       "   Avg_Delta_Composition Primary Column  y:Impurity  \\\n",
       "0                               1.49709     1.77833   \n",
       "1                               1.51392     1.76964   \n",
       "2                               1.50634     1.76095   \n",
       "3                               1.47935     1.75226   \n",
       "4                               1.44489     1.74357   \n",
       "5                               1.51144     1.73488   \n",
       "6                               1.51096     1.72619   \n",
       "7                               1.51180     1.71750   \n",
       "8                               1.48168     1.70881   \n",
       "9                               1.51268     1.70012   \n",
       "\n",
       "   Primary Column Reflux/Feed Ratio  Primary Column Make/Reflux Ratio  \n",
       "0                           3.32803                          0.291226  \n",
       "1                           3.29556                          0.294044  \n",
       "2                           3.23481                          0.300552  \n",
       "3                           3.31287                          0.293752  \n",
       "4                           3.33435                          0.286107  \n",
       "5                           3.31077                          0.290129  \n",
       "6                           3.30961                          0.290165  \n",
       "7                           3.31009                          0.287436  \n",
       "8                           3.32848                          0.288816  \n",
       "9                           3.31516                          0.291564  \n",
       "\n",
       "[10 rows x 46 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "\n",
    "df = pd.read_excel('data/impurity_dataset-training.xlsx')\n",
    "df.head(10) #<- shows the first 10 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with this data we need to \"clean\" it to remove missing values. We will come back to this in the \"data management\" module. For now, just run the cell below and it will create a matrix `X` of inputs and `y` of impurity concentrations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10297, 40) (10297, 1)\n"
     ]
    }
   ],
   "source": [
    "def is_real_and_finite(x):\n",
    "    if not np.isreal(x):\n",
    "        return False\n",
    "    elif not np.isfinite(x):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "all_data = df[df.columns[1:]].values #drop the first column (date)\n",
    "numeric_map = df[df.columns[1:]].applymap(is_real_and_finite)\n",
    "real_rows = numeric_map.all(axis=1).copy().values #True if all values in a row are real numbers\n",
    "X = np.array(all_data[real_rows,:-5], dtype='float') #drop the last 5 cols that are not inputs\n",
    "y = np.array(all_data[real_rows,-3], dtype='float')\n",
    "y = y.reshape(-1,1)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the dataset we will work with. We have 10297 data points, with 40 input variables (features) and one output variable. We can pull the names of the features (and output) in case we forget later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:Impurity\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['x1:Primary Column Reflux Flow',\n",
       " 'x2:Primary Column Tails Flow',\n",
       " 'x3:Input to Primary Column Bed 3 Flow',\n",
       " 'x4:Input to Primary Column Bed 2 Flow',\n",
       " 'x5:Primary Column Feed Flow from Feed Column',\n",
       " 'x6:Primary Column Make Flow',\n",
       " 'x7:Primary Column Base Level',\n",
       " 'x8:Primary Column Reflux Drum Pressure',\n",
       " 'x9:Primary Column Condenser Reflux Drum Level',\n",
       " 'x10:Primary Column Bed1 DP',\n",
       " 'x11:Primary Column Bed2 DP',\n",
       " 'x12:Primary Column Bed3 DP',\n",
       " 'x13:Primary Column Bed4 DP',\n",
       " 'x14:Primary Column Base Pressure',\n",
       " 'x15:Primary Column Head Pressure',\n",
       " 'x16:Primary Column Tails Temperature',\n",
       " 'x17:Primary Column Tails Temperature 1',\n",
       " 'x18:Primary Column Bed 4 Temperature',\n",
       " 'x19:Primary Column Bed 3 Temperature',\n",
       " 'x20:Primary Column Bed 2 Temperature',\n",
       " 'x21:Primary Column Bed 1 Temperature',\n",
       " 'x22: Secondary Column Base Concentration',\n",
       " 'x23: Flow from Input to Secondary Column',\n",
       " 'x24: Secondary Column Tails Flow',\n",
       " 'x25: Secondary Column Tray DP',\n",
       " 'x26: Secondary Column Head Pressure',\n",
       " 'x27: Secondary Column Base Pressure',\n",
       " 'x28: Secondary Column Base Temperature',\n",
       " 'x29: Secondary Column Tray 3 Temperature',\n",
       " 'x30: Secondary Column Bed 1 Temperature',\n",
       " 'x31: Secondary Column Bed 2 Temperature',\n",
       " 'x32: Secondary Column Tray 2 Temperature',\n",
       " 'x33: Secondary Column Tray 1 Temperature',\n",
       " 'x34: Secondary Column Tails Temperature',\n",
       " 'x35: Secondary Column Tails Concentration',\n",
       " 'x36: Feed Column Recycle Flow',\n",
       " 'x37: Feed Column Tails Flow to Primary Column',\n",
       " 'x38: Feed Column Calculated DP',\n",
       " 'x39: Feed Column Steam Flow',\n",
       " 'x40: Feed Column Tails Flow']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_names = [str(x) for x in df.columns[1:41]]\n",
    "y_name = str(df.columns[-3])\n",
    "print(y_name)\n",
    "x_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry if all this code doesn't make sense, we will revisit `pandas` in more detail later. The goal is to predict the output, impurity, as a function of all the input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike working with a single variable where we can plot \"x vs. y\", but it is difficult to get a feel for higher-dimension data since it is hard to visualize. One good thing to start with is looking at histograms of each input variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dimensions: (10297, 40)\n",
      "Feature names: ['x1:Primary Column Reflux Flow', 'x2:Primary Column Tails Flow', 'x3:Input to Primary Column Bed 3 Flow', 'x4:Input to Primary Column Bed 2 Flow', 'x5:Primary Column Feed Flow from Feed Column', 'x6:Primary Column Make Flow', 'x7:Primary Column Base Level', 'x8:Primary Column Reflux Drum Pressure', 'x9:Primary Column Condenser Reflux Drum Level', 'x10:Primary Column Bed1 DP', 'x11:Primary Column Bed2 DP', 'x12:Primary Column Bed3 DP', 'x13:Primary Column Bed4 DP', 'x14:Primary Column Base Pressure', 'x15:Primary Column Head Pressure', 'x16:Primary Column Tails Temperature', 'x17:Primary Column Tails Temperature 1', 'x18:Primary Column Bed 4 Temperature', 'x19:Primary Column Bed 3 Temperature', 'x20:Primary Column Bed 2 Temperature', 'x21:Primary Column Bed 1 Temperature', 'x22: Secondary Column Base Concentration', 'x23: Flow from Input to Secondary Column', 'x24: Secondary Column Tails Flow', 'x25: Secondary Column Tray DP', 'x26: Secondary Column Head Pressure', 'x27: Secondary Column Base Pressure', 'x28: Secondary Column Base Temperature', 'x29: Secondary Column Tray 3 Temperature', 'x30: Secondary Column Bed 1 Temperature', 'x31: Secondary Column Bed 2 Temperature', 'x32: Secondary Column Tray 2 Temperature', 'x33: Secondary Column Tray 1 Temperature', 'x34: Secondary Column Tails Temperature', 'x35: Secondary Column Tails Concentration', 'x36: Feed Column Recycle Flow', 'x37: Feed Column Tails Flow to Primary Column', 'x38: Feed Column Calculated DP', 'x39: Feed Column Steam Flow', 'x40: Feed Column Tails Flow']\n"
     ]
    }
   ],
   "source": [
    "print('X dimensions: {}'.format(X.shape))\n",
    "print('Feature names: {}'.format(x_names))\n",
    "N = X.shape[-1]\n",
    "n = int(np.sqrt(N))\n",
    "fig, axes = plt.subplots(n, n+1, figsize = (5*n, 5*n))\n",
    "ax_list = axes.ravel()\n",
    "for i in range(N):\n",
    "    ax_list[i].hist(X[:,i])\n",
    "    ax_list[i].set_xlabel(x_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some features are normally distributed, while others have some obvious outliers or bimodal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: Why might there by bimodal distributions in a chemical process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look for feature \"correlations\" through the covariance matrix. The covariance explains how features vary with each other. We won't go through the math here, but we will discuss the concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar = np.cov(X.T)\n",
    "fig,ax = plt.subplots()\n",
    "c = ax.imshow(covar)\n",
    "fig.colorbar(c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix tells us that some features seem highly correlated. We can look at some specific entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar[2,2]\n",
    "#covar[2,3]\n",
    "#covar[1,3]\n",
    "covar[1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers don't seem to mean much right now. The \"covariance\" between feature 2 and itself is higher than the covariance between feature 1 and itself. We will return to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Features and Outputs\n",
    "\n",
    "We can see that different features have very different ranges, and different units (e.g. degrees, percent, count).  Scaling data is like \"non-dimensionalizing\" or normalizing for different units. This is often critical to ensure that certain variables are not weighted more than others.\n",
    "\n",
    "Statistical methods don't know about physical units, so we can normalize or \"scale\" features to aid in comparison:\n",
    "\n",
    "* rescaling: 0 = min, 1 = max\n",
    "* mean scaling: 0 = mean, 1 = max, -1 = min\n",
    "* **standard scaling: 0 = mean, 1 = standard deviation**\n",
    "* unit vector: the length of each multi-dimensional vector is 1\n",
    "\n",
    "See the [scikit-learn documentation](http://scikit-learn.org/stable/modules/preprocessing.html) for more examples and discussion.\n",
    "\n",
    "Note that scaling is not always a good idea. Sometimes the data have units that are already consistent, or re-scaling can remove some important aspects. Figuring out the best scaling scheme is often achieved through trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can look at the features and see they clearly have different units, and different ranges. For example, feature 1 (Primary column tails flow) ranges from 0 to 50, and feature 2 (Input to primary column Bed 3 Flow) ranges from 0 to ~3000. While we don't necessarily know the units (since this is proprietary data), we can see that there is a difference of range. This is why the covariance matrix didn't make much sense. We can rescale the data to put everything on similar scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: What could go wrong with rescaling or mean scaling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_scaled = (X - X.mean(axis=0))/X.std(axis=0)\n",
    "print(\"Minimum: {}, Maximum: {}\".format(X.min(), X.max()))\n",
    "print(\"Minimum scaled: {}, Maximum scaled: {}\".format(X_scaled.min(), X_scaled.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can re-compute the covariance matrix with the rescaled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar = np.cov(X_scaled.T)\n",
    "fig,ax = plt.subplots()\n",
    "c = ax.imshow(covar)\n",
    "fig.colorbar(c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure looks totally different! This is the \"correlation matrix\", which tells us how correlated different features are on a scale of -1 to 1. A correlation of -1 means they are perfectly anti-correlated, while 1 means they are perfectly correlated. If any features are perfectly correlated then they are linearly dependent (and won't count toward the rank)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the rank is 40, but the maximum covariance is 1. The reason is that the diagonal entries of the covariance matrix will always be 1 since features are perfectly correlated with themselves.\n",
    "\n",
    "If the data has been \"standard scaled\" then the covariance matrix will range from -1 to 1, and is equivalent to a correlation matrix, which can also be computed directly from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(X.T)\n",
    "covar = np.cov(X_scaled.T)\n",
    "np.isclose(corr, covar, 1e-4).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will discuss the covariance/correlation matrix much more later, but when dealing with multi-dimensional data it is always good to check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Linear Regression\n",
    "\n",
    "We can recall the general form of a linear regression model:\n",
    "\n",
    "$y_i = \\sum_j w_j X_{ij} + \\epsilon_i$\n",
    "\n",
    "Previously, we created features (columns of $X$) by transforming the original 1-dimensional input. In this case, we already have columns of $X$ provided from the data. We can use the same linear regression techniques from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression() #create a linear regression model instance\n",
    "model.fit(X_scaled, y) #fit the model\n",
    "r2 = model.score(X_scaled, y) #get the \"score\", which is equivalent to r^2\n",
    "\n",
    "yhat = model.predict(X_scaled) #create the model prediction\n",
    "\n",
    "print(\"r^2 = {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the $r^2$ score is 0.71, which is not terrible, but not great either.  We can't really visualize the model since we have 40-dimensional inputs. However, we can make a \"parity plot\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(y, yhat,alpha=0.15)\n",
    "ax.plot(y,y, '-k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks reasonable, although there are quite a few outliers. We should also remember that we trained on all the data, so this might be over-fit. We can quickly test using hold out cross validation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3)\n",
    "\n",
    "model = LinearRegression() #create a linear regression model instance\n",
    "model.fit(X_train, y_train) #fit the model to training data\n",
    "r2_train = model.score(X_train, y_train) #get the score for training data\n",
    "\n",
    "yhat = model.predict(X_test) #create the model prediction\n",
    "r2_test = model.score(X_test, y_test) #get the score for testing data\n",
    "\n",
    "print(\"r^2 train = {}\".format(r2_train))\n",
    "print(\"r^2 test = {}\".format(r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that they are comparable, which indicates that we have not over-fit. We can also visualize both training and testing errors with a parity plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "yhat_train = model.predict(X_train)\n",
    "ax.scatter(y_train, yhat_train,alpha=0.1, c='b')\n",
    "ax.scatter(y_test, yhat,alpha=0.1, c='r')\n",
    "\n",
    "ax.plot(y,y, '-k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these look comparable, which confirms that we have not over-fit the model. It is always a good idea to check the parity plot to see if any patterns stand out!\n",
    "\n",
    "This basic linear regression model is simple, but by testing it we now have a **baseline model**. This tells us that if we have any results worse than this we have a really bad model! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the performance of the model is not great, and to improve things we will need to add some non-linearity. In 1-dimensional space we achieved this by adding transforms of the features as new features. However, for this is more challenging in a high-dimensional space since the number of features will scale with the number of dimension.\n",
    "\n",
    "### Discussion: How many features would result if third-order interactions were considered?\n",
    "\n",
    "Kernel-based methods are very commonly used for high-dimensional spaces because they account for non-linear interactions, but the number of features does not exceed the number of data points. In your homework you will explore the application of KRR to this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "An alternative approach to creating high-dimensional models is to reduce the dimensionality. We will briefly look at some techniques here, and revisit this idea later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Selection\n",
    "\n",
    "The simplest strategy to select or rank features is to try them one-by-one, and keep the best feature at each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_features = 4\n",
    "X_subset = X_scaled.copy()\n",
    "x_names_subset = np.copy(x_names)\n",
    "new_X = []\n",
    "new_X_names = []\n",
    "\n",
    "while len(new_X) < N_features:\n",
    "    r2_list = []\n",
    "    for j in range(X_subset.shape[1]):\n",
    "        model = LinearRegression() #create a linear regression model instance\n",
    "        xj = X_subset[:,j].reshape(-1,1)\n",
    "        model.fit(xj, y) #fit the model\n",
    "        r2 = model.score(xj, y) #get the \"score\", which is equivalent to r^2\n",
    "        r2_list.append([r2, j])\n",
    "    r2_list.sort() #sort lowest to highest\n",
    "    r2_max, j_max = r2_list[-1] #select highest r2 value\n",
    "    new_X.append(X_subset[:,j_max].copy())\n",
    "    new_X_names.append(x_names_subset[j_max])\n",
    "    np.delete(x_names_subset, j_max)\n",
    "    X_subset = np.delete(X_subset, j_max, axis=1)\n",
    "    \n",
    "print('The {} most linearly correlated features are: {}'.format(N_features, new_X_names))\n",
    "\n",
    "new_X = np.array(new_X).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the $r^2$ score changes with the reduced features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression() #create a linear regression model instance\n",
    "model.fit(new_X, y) #fit the model\n",
    "r2 = model.score(new_X, y) #get the \"score\", which is equivalent to r^2\n",
    "print(\"r^2 = {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with just 4 features the model performance is substantially reduced. We can keep increasing the number until it is comparable to the full model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Use forward selection to determine the minimum number of features needed to get an $r^2=0.65$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful, since just because features are *linearly* correlated does not mean that they are *non-linearly* correlated. There is also no guarantee that we are not finding correlated features, since if one feature has a high correlation with the output, and is also correlated with another feature, then that feature will also be correlated with the output. One way to avoid this is to ensure that features are orthogonal using the eigenvectors of the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import eigvals, eig\n",
    "\n",
    "eigvals, eigvecs = eig(corr)\n",
    "\n",
    "print(eigvals)\n",
    "print(np.dot(eigvecs[:,1], eigvecs[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that by taking the eigenvalues of the covariance matrix you are actually doing something called \"principal component analysis\". The eigenvectors of the covariance matrix identify the \"natural\" coordinate system of the data.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/PCA.gif\" width=\"500\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalues provide the variance in each direction, and we can use this to determine how much variance each principal component contributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCvals, PCvecs = eigvals, eigvecs\n",
    "total_variance = np.sum(np.real(PCvals))\n",
    "explained_variance = np.real(PCvals)/total_variance\n",
    "print(total_variance)\n",
    "print(explained_variance)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(explained_variance, 'o')\n",
    "ax.plot(np.cumsum(explained_variance),'or')\n",
    "ax.plot([0,10],[0.9, 0.9]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to say that 90% of the variance in the data is explained by the first 7 principal components.\n",
    "\n",
    "Finally, we can \"project\" the data onto the principal components. This is equivalent to re-defining the axes of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_projection = np.dot(X_scaled, PCvecs)\n",
    "print(PC_projection.shape)\n",
    "\n",
    "corr_PCs = np.corrcoef(PC_projection.T)\n",
    "fig,ax = plt.subplots()\n",
    "c = ax.imshow(corr_PCs)\n",
    "fig.colorbar(c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After projection, we still have 40 features but they are now orthogonal - there is no covariance! This means that each one contains unique information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will talk a lot more about PCA throughout the course, but for now you should know:\n",
    "\n",
    "* Principal component vectors are obtained from the eigenvalues of the covariance matrix\n",
    "* Principal components are orthogonal\n",
    "* Principal components explain the variance in multi-dimensional data\n",
    "* Data can be projected onto principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the projected data as inputs to a regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression() #create a linear regression model instance\n",
    "model.fit(PC_projection, y) #fit the model\n",
    "r2 = model.score(PC_projection, y) #get the \"score\", which is equivalent to r^2\n",
    "print(\"r^2 = {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression() #create a linear regression model instance\n",
    "model.fit(X_scaled, y) #fit the model\n",
    "r2 = model.score(X_scaled, y) #get the \"score\", which is equivalent to r^2\n",
    "print(\"r^2 = {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the answer is the same. This is because we are still ultimately including all the same information. However, if we want to reduce the number of features we will see a difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8\n",
    "\n",
    "model_PC = LinearRegression() #create a linear regression model instance\n",
    "model_PC.fit(PC_projection[:, :N], y) #fit the model\n",
    "r2 = model_PC.score(PC_projection[:, :N], y) #get the \"score\", which is equivalent to r^2\n",
    "print(\"r^2 PCA = {}\".format(r2))\n",
    "\n",
    "model = LinearRegression() #create a linear regression model instance\n",
    "model.fit(X_scaled[:, :N], y) #fit the model\n",
    "r2 = model.score(X_scaled[:, :N], y) #get the \"score\", which is equivalent to r^2\n",
    "print(\"r^2 regular = {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: Why is the model with principal components not always better than direct linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA projection collects as much information as possible in each feature, and orders them by the amount of variance. We can also check them one-by-one to see how they correlate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_list = []\n",
    "for j in range(PC_projection.shape[1]):\n",
    "    model = LinearRegression() #create a linear regression model instance\n",
    "    xj = PC_projection[:,j].reshape(-1,1)\n",
    "    model.fit(xj, y) #fit the model\n",
    "    r2 = model.score(xj, y) #get the \"score\", which is equivalent to r^2\n",
    "    score_list.append([r2, j])\n",
    "score_list.sort()\n",
    "score_list.reverse()\n",
    "\n",
    "for r, j in score_list:\n",
    "    print(\"{} : r^2 = {}\".format(j, r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the second principal component is actually the best, the first is the second best, and the seventh is third. This is because the principal components only use variance of the inputs, which may or may not correlate to the outputs.\n",
    "\n",
    "It is common to use PCA or other dimensionality techniques prior to regression when working with high-dimensional data. It is often possible to construct models that have better performance with fewer input dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
